# personal

Excellent â€” this is exactly how you should frame your **Copilot-guided build** for maximum precision. Below is a **two-file setup**:

1. `COPILOT_INSTRUCTIONS.md` â†’ Tells GitHub Copilot *how to behave* (its persona, architecture decisions, priorities).
2. `INSTRUCTIONS.md` â†’ Tells **you + Copilot** *what to build step-by-step* (your project plan).

This combination will help Copilot scaffold your **Agentic Knowledge Graph Builder for Financial Data Products** using **LangChain/LangGraph, SQLAlchemy, and Neo4j**, directly connecting to **Databricks Delta tables (EDL)** instead of CSVs.

---

# ğŸ§­ `COPILOT_INSTRUCTIONS.md`

> Place this file at the root of your repo. Copilot reads it as guidance for code generation and architectural consistency.

````markdown
# GitHub Copilot Custom Instructions
You are assisting in building a project called **Agentic Knowledge Graph Builder for Financial Data Products**.

## ğŸ¯ Goal
Build a structured-only, agentic knowledge graph system that connects **financial data products** (Customers, Agreements, Payments, Collateral) stored in **Databricks Delta tables** via SQLAlchemy, into a unified **Neo4j knowledge graph**, using **LangGraph + LangChain + OpenAI APIs**.

The system should be modular, agentic, and easily extensible.

---

## ğŸ§© Architecture Overview

**Core Components**
- Agents for:
  - Intent understanding
  - Table/schema discovery via SQLAlchemy (Databricks)
  - Schema proposal & refinement
  - Graph builder (Neo4j)
  - Query interface (NL â†’ Cypher)
- LangGraph orchestrates the workflow between agents.
- Streamlit UI for demo and hackathon presentation.

**Flow**
1. User provides intent (e.g. â€œconnect customer, agreements, paymentsâ€).
2. System inspects Databricks tables (via SQLAlchemy) â†’ extracts schema.
3. Schema proposal agent infers node and relationship types.
4. Graph builder agent creates nodes/edges in Neo4j.
5. Query agent converts NL to Cypher for exploration.

---

## ğŸ§  Key Libraries & Setup

- `langchain`, `langgraph`, `openai`, `neo4j`, `sqlalchemy`, `pandas`, `streamlit`, `python-dotenv`
- Connect to **Databricks** via:
  ```python
  engine = create_engine("databricks+connector://token:<DATABRICKS_TOKEN>@<HOST>:443/<CATALOG>.<SCHEMA>")
````

* Access Delta table schema with SQLAlchemy reflection.
* Insert data into Neo4j using the `neo4j` Python driver.

---

## ğŸ§± Folder Structure (Copilot must maintain)

```
agentic-kg-finance/
â”‚â”€â”€ README.md
â”‚â”€â”€ docker-compose.yml
â”‚â”€â”€ .env.example
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â”œâ”€â”€ config.py
â”‚   â”‚
â”‚   â”œâ”€â”€ agents/
â”‚   â”‚   â”œâ”€â”€ intent_agent.py
â”‚   â”‚   â”œâ”€â”€ table_discovery_agent.py
â”‚   â”‚   â”œâ”€â”€ schema_proposal_agent.py
â”‚   â”‚   â”œâ”€â”€ graph_builder_agent.py
â”‚   â”‚   â””â”€â”€ query_agent.py
â”‚   â”‚
â”‚   â”œâ”€â”€ workflows/
â”‚   â”‚   â””â”€â”€ structured_pipeline.py
â”‚   â”‚
â”‚   â”œâ”€â”€ ui.py
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ databricks_connector.py
â”‚       â””â”€â”€ neo4j_helper.py
```

---

## âš™ï¸ Copilot Implementation Guidelines

* Each agent should be implemented as a **class with a `run()` method**.
* Use **dependency injection** for shared configs (Databricks/Neo4j connections).
* Add **docstrings** explaining agent purpose and input/output clearly.
* Code must use **async/await** where useful for API calls.
* Follow **clean architecture principles**:

  * Agents in `agents/`
  * Orchestration in `workflows/`
  * External connections in `utils/`
* Use **logging**, not prints, for internal debugging.
* Always include **type hints** for function signatures.

---

## ğŸ§© LangGraph Integration

Copilot should use `StateGraph` from `langgraph.graph` to connect the following agent nodes:

```
IntentAgent â†’ TableDiscoveryAgent â†’ SchemaProposalAgent â†’ GraphBuilderAgent â†’ QueryAgent
```

Each node should pass context/state (intent, schema info, etc.) forward.

---

## ğŸ§ª Example Prompt Behavior

When user says:

> "Build a KG linking customers, agreements, and payments."

Expected flow:

1. `IntentAgent` â†’ extracts â€œconnect customers, agreements, paymentsâ€.
2. `TableDiscoveryAgent` â†’ queries Databricks catalog for table metadata.
3. `SchemaProposalAgent` â†’ infers relationships (CUSTOMER_HAS_AGREEMENT, AGREEMENT_HAS_PAYMENT).
4. `GraphBuilderAgent` â†’ inserts nodes/edges into Neo4j.
5. `QueryAgent` â†’ enables Cypher/NL queries.

---

## ğŸš€ Copilot Output Quality Targets

* Use **modular**, reusable functions.
* Generate minimal, readable, production-quality code.
* Avoid mockups or print-based logic unless explicitly testing.
* Prefer explicit connection handling (no globals).
* Include fallback logic for missing FKs or inconsistent schemas.
* Code must be **runnable in Docker**.

---

## ğŸ”’ Security & Credentials

* Load secrets from `.env`
* Never hardcode Databricks or Neo4j credentials.
* Respect the config file structure (`config.py`).

---

## âœ… Deliverables

Copilot should generate:

1. Functional pipeline that connects to Databricks â†’ infers schemas â†’ builds Neo4j KG.
2. Streamlit app for end-user query and visualization.
3. Reusable agents and workflow logic.

````

---

# ğŸ§­ `INSTRUCTIONS.md`

> This file defines *your build plan and step-by-step tasks for Copilot*.

```markdown
# ğŸ—ï¸ Agentic Knowledge Graph Builder (Financial Data Products)

This document defines the **build plan** for constructing the project using GitHub Copilot.  
Each step is atomic â€” ask Copilot to implement one file or function at a time.

---

## ğŸ”¹ Step 1: Setup Environment

- Create `agentic-kg-finance/` project folder.
- Add `.env.example`:
  ```env
  OPENAI_API_KEY=your_key_here
  DATABRICKS_HOST=<your_host>
  DATABRICKS_TOKEN=<your_token>
  DATABRICKS_CATALOG=<catalog>
  DATABRICKS_SCHEMA=<schema>
  NEO4J_URI=bolt://neo4j:7687
  NEO4J_USER=neo4j
  NEO4J_PASSWORD=password
````

* Add `requirements.txt`:

  ```
  langchain
  langgraph
  openai
  neo4j
  sqlalchemy
  pandas
  streamlit
  python-dotenv
  ```

---

## ğŸ”¹ Step 2: Implement Configuration

* Create `app/config.py` to load `.env` and expose environment variables.
* Add a function to return Databricks SQLAlchemy engine and Neo4j driver.

---

## ğŸ”¹ Step 3: Create Databricks Connector Utility

File: `app/utils/databricks_connector.py`

* Use SQLAlchemyâ€™s reflection:

  ```python
  from sqlalchemy import create_engine, inspect
  engine = create_engine(f"databricks+connector://token:{token}@{host}:443/{catalog}.{schema}")
  inspector = inspect(engine)
  tables = inspector.get_table_names()
  columns = inspector.get_columns(table_name)
  ```
* Return a dictionary of `{table_name: [column metadata]}`.

---

## ğŸ”¹ Step 4: Create Neo4j Helper

File: `app/utils/neo4j_helper.py`

* Initialize Neo4j driver.
* Add helper functions:

  * `create_node(label, properties)`
  * `create_relationship(parent_label, child_label, parent_id, child_id, rel_type)`
  * `run_cypher(query)`

---

## ğŸ”¹ Step 5: Implement Agents

1. **IntentAgent**

   * Extract goal and entities from natural language prompt.
2. **TableDiscoveryAgent**

   * Map discovered entities (from IntentAgent) to Databricks table names.
3. **SchemaProposalAgent**

   * Infer possible relationships and graph schema based on metadata.
4. **GraphBuilderAgent**

   * Create nodes/relationships in Neo4j.
5. **QueryAgent**

   * Translate NL queries â†’ Cypher (use OpenAI API).

---

## ğŸ”¹ Step 6: Implement LangGraph Workflow

File: `app/workflows/structured_pipeline.py`

* Create a `StateGraph` with:

  ```
  IntentAgent â†’ TableDiscoveryAgent â†’ SchemaProposalAgent â†’ GraphBuilderAgent â†’ QueryAgent
  ```
* Pass shared state between nodes.
* Log outputs for debugging.

---

## ğŸ”¹ Step 7: Add Streamlit UI

File: `app/ui.py`

* Input: user intent / natural language question.
* Buttons:

  * â€œBuild KGâ€
  * â€œQuery KGâ€
* Display:

  * Agent responses
  * Cypher queries
  * JSON / tabular output.

---

## ğŸ”¹ Step 8: Dockerize

File: `docker-compose.yml`

* Services:

  * `neo4j`
  * `app` (Python + Streamlit)
* Mount `/app` for hot reload.
* Include `.env`.

---

## ğŸ”¹ Step 9: Test Plan

* Test intent parsing with mock prompts.
* Test schema discovery with real Databricks tables.
* Test Neo4j insertions and Cypher queries.
* Ensure full pipeline runs end-to-end.

---

## ğŸ”¹ Step 10: Demo Script

1. Start Docker.
2. In Streamlit, type:
   *â€œI want to build a KG connecting customers, agreements, and payments.â€*
3. System fetches Databricks table metadata â†’ proposes schema â†’ builds Neo4j graph.
4. Ask:
   *â€œWhich customers have overdue payments?â€*
5. System generates Cypher and displays result.

---

## ğŸ”¹ Step 11: Hackathon Submission Prep

Include:

* README with architecture diagram and description.
* Screenshot of Streamlit UI and Neo4j graph view.
* Summary of agent flow.

```

---

Would you like me to add a **sample `.env.example`** and **databricks connector string template** pre-filled with placeholders for your EDL so you can drop it directly into your environment?
```
